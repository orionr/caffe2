[["dec_out", [6, 1, 44463], "float32"], ["avg_attn", [6, 1, 20], "float32"], ["scores_t", [1, 6], "float32"]]
beam_search
["scores_t"]

def beam_search(dec_out, avg_attn, scores_t) -> ():
    timestep = 0
    max_out_seq_len = 20
    while timestep < max_out_seq_len:
        # TODO: once we have a metaprogramming facility we need to insert the
        # body of the post_eos_penalty here programmatically

        best_scores_per_hypo, best_tokens_per_hypo = dec_out.TopK(k=6)

        # Add the best score in each hypothesis to the cumulative score so far
        output_scores = best_scores_per_hypo + scores_t.Squeeze(dims=[0])

        # Flatten scores so we can find the best overall out of all hypotheses
        output_scores_flattened_slice, _ = output_scores.FlattenToVec()\
            .Slice(0, 6 if timestep == 0 else -1).Reshape(shape=[1, -1])

        # Find top K out of all
        scores_t, best_indices = output_scores_flattened_slice.TopK(k=6)

        # Integer floor divide on indices finds the association back to original
        #  hypotheses. Use this to reorder states
        hypo_t_int64 = best_indices / 6LL

        # Reorder attentions
        attention_t, _ = avg_attn.Gather(hypo_t_int64)\
            .Reshape(shape=[1, 6, -1])
        tokens_t_int64 = best_tokens_per_hypo.FlattenToVec()\
            .Gather(best_indices).Cast(to=2)

        timestep += 1
